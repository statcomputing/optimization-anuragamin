---
title: "Optimization and Population Modeling"
author: 
  - Anurag Amin^[<anurag.amin@uconn.edu>; Master's student at
    Department of Mathematics, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  bookdown::html_document2
  bookdown::pdf_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

  In this homework we will complete three problems, first deriving foundational results concerning maximum  likelihood estimators and resultant expectancy, and then analyzing a population model generated by a differential equation. After the completing these derivations, we will test the accuracy of these estimates using an artificially provided sample. Graphs will be implemented periodically to aid in our analysis.
  
# Problem 1

(a)

\begin{equation}
\begin{split}
\\

\ell(\theta) &= ln(L(\theta))\\
             &= ln\prod\limits_{i=1}^n f(x_{i}|\theta)\\
             &= \sum\limits_{i=1}^n ln(f(x_{i}|\theta))\\
             &= \sum\limits_{i=1}^n ln\frac{1}{\pi[1 +(x_{i} - \theta)^2]}\\
             &= \sum\limits_{i=1}^n -ln[1 + (\theta - x_{i})^2] - ln(\pi)\\
\ell(\theta) &= -ln(\pi)n - \sum\limits_{i=1}^n ln[1 + (\theta - x_{i})^2]\\
\\

\ell'(\theta) &= \sum\limits_{i=1}^n [ln(1 + (\theta - x_{i})^2)]'\\
              &= -\sum\limits_{i=1}^n \frac{2(\theta - x_{i})}{1 + (\theta - x_{i})^2}\\
\ell'(\theta) &= -2\sum\limits_{i=1}^n \frac{(\theta - x_{i})}{1 + (\theta - x_{i})^2}\\
\\

\ell''(\theta) &= -2\sum\limits_{i=1}^n \frac{(1)(1 + (\theta - x_{i})^2)) - 2(\theta - x_{i})^2)}{[1 + (\theta - x_{i})^2]^2}\\
               &= -2\sum\limits_{i=1}^n \frac{1 - (\theta - x_{i})^2}{[1 + (\theta - x_{i})^2)]^2}\\
\\

I(\theta) &= n\int \cfrac{[p'(x)]^2}{p(x)}\\
          &= \cfrac{4n}{\pi} \int_{-\infty}^{\infty} \cfrac{x^2dx}{(1+x^2)^3}\\
\\
\end{split}
\end{equation}

\begin{align}
Let\hspace{1mm}us\hspace{1mm} substitute\hspace{2mm} x = \tan(t)\hspace{2mm} where\hspace{2mm} t\in (-\cfrac{\pi}{2},\cfrac{\pi}{2}), \hspace{1mm}we\hspace{1mm} get:\\
\end{align}

\begin{equation}
\begin{split}

I(\theta) &= \cfrac{4n}{\pi} \int_{-\cfrac{\pi}{2}}^{\cfrac{\pi}{2}} \cfrac{\tan^2(t)d(\tan(t))}{(1+tan^2(t))^3}\\
          &= \cfrac{4n}{\pi} \int_{-\cfrac{\pi}{2}}^{\cfrac{\pi}{2}} \cfrac{\tan^2(t)}{(\cfrac{1}{\sec^2(t)})^3}(\cfrac{1}{\sec^2(t)})dt\\
          &= \cfrac{4n}{\pi} \int_{-\cfrac{\pi}{2}}^{\cfrac{\pi}{2}} \cfrac{\sin^2(t)}{\cos^2(t)}\cos^4(t)dt\\
          &= \cfrac{4n}{\pi} \int_{-\cfrac{\pi}{2}}^{\cfrac{\pi}{2}}\sin^2(t)\cos^2(t)dt\\
          &= \cfrac{4n}{\pi}*\cfrac{\pi}{8}=\cfrac{n}{2}\\
\\          
\end{split}
\end{equation}


(b)

Now we will graph the log-likelihood function and find the MLE for $\theta$ using Newton-Raphson method.

We are given this observed sample -

x <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)

Here is the graph of the log-likelihood function -

```{r, eval=TRUE, echo=FALSE, warning=FALSE}

#Given sample
x <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)

#Consider this function base,
base <- function(theta,x) {
  -log(pi)-log(1+(theta-x)^2)
}

log_likelihood <- function(theta) {
    base(theta,1.77) + base(theta,-0.23) + base(theta,2.76) + base(theta,3.80) + base(theta,3.47) + base(theta,56.75) + base(theta,-1.34) + base(theta,4.24) + base(theta,-2.44) + base(theta,3.29) + base(theta,3.71) + base(theta,-2.40) + base(theta,4.53) + base(theta,-0.07) + base(theta,-1.05) + base(theta,-13.87) + base(theta,-2.53) + base(theta,-1.75)
}
curve(log_likelihood, from=-5, to=5, main = "Graph for the log-likelihood function")
```

Now, we will repeat the process for various given starting points - 

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

sp1 <- nlminb(-11,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with -11 as the starting point: ",sp1$iterations)
sp2 <- nlminb(-1,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with -1 as the starting point: ",sp2$iterations)
sp3 <- nlminb(0,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 0 as the starting point: ",sp3$iterations)
sp4 <- nlminb(1.5,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 1.5 as the starting point: ",sp4$iterations)
sp5 <- nlminb(4,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 4 as the starting point: ",sp5$iterations)
sp6 <- nlminb(4.7,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 4.7 as the starting point: ",sp6$iterations)
sp7 <- nlminb(7,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 7 as the starting point: ",sp7$iterations)
sp8 <- nlminb(8,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 8 as the starting point: ",sp8$iterations)
sp9 <- nlminb(38,log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with 38 as the starting point: ",sp9$iterations)

```

Let's see if the mean of the sample space is a good starting point -  
```{r, eval=TRUE, echo=FALSE, warning=FALSE}
average <- mean(x)
spmean <- nlminb(average, log_likelihood, control=list(eval.max=1000, iter.max=1000))
cat("Number of iterations required for convergence with the mean of the sample as the starting point: ",spmean$iterations)
```

As we can see from the results, number of iterations required to achieve convergence for most of the given starting points are very high (in the 700s). Even taking the sample mean as the starting point didn't help. However, taking 0 as the starting point only required 4 iterations. Hence, we can conclude that 0 is the best starting point for our analysis.

(c)

Now, we will try the fixed-point method to carry out our iterations. Here's the code chunk and solution -

```{r, echo=TRUE, eval=TRUE, warning=FALSE}

fixedpointiterations <- function(fun, x0, tol=.00000001, niter=500){
  oldxvalue <- x0
  newxvalue <- fun(oldxvalue)
  for(i in 1:niter){
    oldxvalue <- newxvalue
    newxvalue <- fun(oldxvalue)
    if(abs((newxvalue - oldxvalue)) < tol)
      return(newxvalue)
  }
  stop
  return(NULL)
}

dlogp <- function(theta, x) {
  -(2 * (theta-x) / (1+(theta-x)^2))
}

Galpha1 <- function(theta) {
  1 * (dlogp(theta, 1.77) + dlogp(theta, -.23) + dlogp(theta, 2.76) + dlogp(theta, 3.80) + dlogp(theta, 3.47) + dlogp(theta, 56.75) + dlogp(theta, -1.34) + dlogp(theta, 4.24) + dlogp(theta, -2.44) + dlogp(theta, 3.29) + dlogp(theta, 3.71) + dlogp(theta, -2.40) + dlogp(theta, 4.53) + dlogp(theta, -.07) + dlogp(theta, -1.05) + dlogp(theta, -13.87) + dlogp(theta, -2.53) + dlogp(theta, -1.75)) + theta 
}

fix1_negative11 <- fixedpointiterations(Galpha1, -11)
fix1_negative11
fix1_negative1 <- fixedpointiterations(Galpha1, -1)
fix1_negative1
fix1_0 <- fixedpointiterations(Galpha1, 0)
fix1_0
fix1_1.5 <- fixedpointiterations(Galpha1, 1.5)
fix1_1.5
fix1_4 <- fixedpointiterations(Galpha1, 4)
fix1_4
fix1_4.7 <- fixedpointiterations(Galpha1, 4.7)
fix1_4.7
fix1_7 <- fixedpointiterations(Galpha1, 7)
fix1_7
fix1_8 <- fixedpointiterations(Galpha1, 8)
fix1_8
fix1_38 <- fixedpointiterations(Galpha1, 38)
fix1_38

Galpha0.64 <- function(theta) {
  0.64 * (dlogp(theta, 1.77) + dlogp(theta, -.23) + dlogp(theta, 2.76) + dlogp(theta, 3.80) + dlogp(theta, 3.47) + dlogp(theta, 56.75) + dlogp(theta, -1.34) + dlogp(theta, 4.24) + dlogp(theta, -2.44) + dlogp(theta, 3.29) + dlogp(theta, 3.71) + dlogp(theta, -2.40) + dlogp(theta, 4.53) + dlogp(theta, -.07) + dlogp(theta, -1.05) + dlogp(theta, -13.87) + dlogp(theta, -2.53) + dlogp(theta, -1.75)) + theta 
}

fix0.64_negative11 <- fixedpointiterations(Galpha0.64, -11)
fix0.64_negative11
fix0.64_negative1 <- fixedpointiterations(Galpha0.64, -1)
fix0.64_negative1
fix0.64_0 <- fixedpointiterations(Galpha0.64, 0)
fix0.64_0
fix0.64_1.5 <- fixedpointiterations(Galpha0.64, 1.5)
fix0.64_1.5
fix0.64_4 <- fixedpointiterations(Galpha0.64, 4)
fix0.64_4
fix0.64_4.7 <- fixedpointiterations(Galpha0.64, 4.7)
fix0.64_4.7
fix0.64_7 <- fixedpointiterations(Galpha0.64, 7)
fix0.64_7
fix0.64_8 <- fixedpointiterations(Galpha0.64, 8)
fix0.64_8
fix0.64_38 <- fixedpointiterations(Galpha0.64, 38)
fix0.64_38

Galpha0.25 <- function(theta) {
  0.25 * (dlogp(theta, 1.77) + dlogp(theta, -.23) + dlogp(theta, 2.76) + dlogp(theta, 3.80) + dlogp(theta, 3.47) + dlogp(theta, 56.75) + dlogp(theta, -1.34) + dlogp(theta, 4.24) + dlogp(theta, -2.44) + dlogp(theta, 3.29) + dlogp(theta, 3.71) + dlogp(theta, -2.40) + dlogp(theta, 4.53) + dlogp(theta, -.07) + dlogp(theta, -1.05) + dlogp(theta, -13.87) + dlogp(theta, -2.53) + dlogp(theta, -1.75)) + theta 
}

fix0.25_negative11 <- fixedpointiterations(Galpha0.25, -11)
fix0.25_negative11
fix0.25_negative1 <- fixedpointiterations(Galpha0.25, -1)
fix0.25_negative1
fix0.25_0 <- fixedpointiterations(Galpha0.25, 0)
fix0.25_0
fix0.25_1.5 <- fixedpointiterations(Galpha0.25, 1.5)
fix0.25_1.5
fix0.25_4 <- fixedpointiterations(Galpha0.25, 4)
fix0.25_4
fix0.25_4.7 <- fixedpointiterations(Galpha0.25, 4.7)
fix0.25_4.7
fix0.25_7 <- fixedpointiterations(Galpha0.25, 7)
fix0.25_7
fix0.25_8 <- fixedpointiterations(Galpha0.25, 8)
fix0.25_8
fix0.25_38 <- fixedpointiterations(Galpha0.25, 38)
fix0.25_38

```

(d)

Soulution and code chunk using Fisher scoring method - 

```{r, echo=TRUE, eval=TRUE, warning=FALSE}

dlogp <- function(theta, x) {
  -( 2 * (theta - x) / (1 + (theta - x)^2))
}

loglikelihood <- function(theta) {
  -(base(theta, 1.77) + base(theta, -.23) + base(theta, 2.76) + base(theta, 3.80) + base(theta, 3.47) + base(theta, 56.75) + base(theta, -1.34) + base(theta, 4.24) + base(theta, -2.44) + base(theta, 3.29) + base(theta, 3.71) + base(theta, -2.40) + base(theta, 4.53) + base(theta, -.07) + base(theta, -1.05) + base(theta, -13.87) + base(theta, -2.53) + base(theta, -1.75))
}

derivativeloglikelihood <- function(theta) {
  (dlogp(theta, 1.77) + dlogp(theta, -0.23) + dlogp(theta, 2.76) +
  dlogp(theta, 3.80) + dlogp(theta, 3.47) + dlogp(theta, 56.75) +
  dlogp(theta, -1.34) + dlogp(theta, 4.24) + dlogp(theta, -2.44) +
  dlogp(theta, 3.29) + dlogp(theta, 3.71) + dlogp(theta, -2.40) +
  dlogp(theta, 4.53) + dlogp(theta, -0.07) + dlogp(theta, -1.05) +
  dlogp(theta, -13.87) + dlogp(theta, -2.53) + dlogp(theta, -1.75))
}

A <- function(x) diag(9, nrow = length(x))

startingpoint_negative11 <- nlminb(-11, loglikelihood, derivativeloglikelihood, A)
startingpoint_negative11
startingpoint_negative1 <- nlminb(-1, loglikelihood, derivativeloglikelihood, A)
startingpoint_negative1
startingpoint_0 <- nlminb(0, loglikelihood, derivativeloglikelihood, A)
startingpoint_0
startingpoint_1.5 <- nlminb(1.5, loglikelihood, derivativeloglikelihood, A)
startingpoint_1.5
startingpoint_4 <- nlminb(4, loglikelihood, derivativeloglikelihood, A)
startingpoint_4
startingpoint_4.7 <- nlminb(4.7, loglikelihood, derivativeloglikelihood, A)
startingpoint_4.7
startingpoint_7 <- nlminb(7, loglikelihood, derivativeloglikelihood, A)
startingpoint_7
startingpoint_8 <- nlminb(8, loglikelihood, derivativeloglikelihood, A)
startingpoint_8
startingpoint_38 <- nlminb(38, loglikelihood, derivativeloglikelihood, A)
startingpoint_38

```

(e)

As evident from the results, the method described in part (d) seems to be providing the most accurate results. However, the method in part (b) is the quickest.


# Problem 2

(a)

$$p(x;\theta)=\frac{1-\cos(x_i-\theta)}{2\pi}$$
$$l(\theta)=\prod_{i=1}^n \frac{1-\cos(x_i-\theta)}{2\pi}=\sum_{i=1}^n \ln{(1-\cos(x_i-\theta))}-n\ln{2\pi}$$

This is the log-likelihood function of $\theta$
Since the derivative of cos(x) is -sin(x), the derivative of the log-likelihood function will be -

$$l^{'}(\theta)=\sum_{i=1}^n \frac{\sin(\theta-x_i)}{1-\cos(\theta-x_i)}$$

$$l^{''}(\theta)=\sum\limits_{i=1}^n \frac{\cos(\theta-x_i)(1-cos(\theta-x_i))-\sin^2(\theta-x_i)}{(1-\cos(\theta-x_i))^2}=\sum_{i=1}^n \frac{\cos(\theta-x_i)-1}{(1-\cos(\theta-x_i))^2}=\sum_{i=1}^n \frac{1}{\cos(theta-x_i)}$$

The given sample is -

x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)

The mean of the sample is -
mean(x) = 3.236842

Now plotting $\ell(\theta)$ -

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)

theta <- seq(-pi,pi, by=pi/100)
loglikelihood <- function(x, theta) {
  sapply(theta, function(theta) sum(log((1-cos(x-theta))/(2*pi))))
}
plot(theta,loglikelihood(x,theta),xlab="Theta",ylab="Log-likelihood of theta",type="l")

```

(b)

Here, we are asked to find the method-of-moments estimator of $\theta$

$$E[X|\theta]=\frac{1}{2\pi}\int_{0}^{2\pi}x*({1-\cos(x-\theta)})dx=\pi-\frac{1}{2\pi}\int_{0}^{2\pi}x*\cos(x-\theta)dx$$

Integrating by parts yields the following:

$$\int_{0}^{2\pi}x*\cos(x-\theta)dx=x*\sin(x-\theta)|_{0}^{2\pi}-\int_{0}^{2\pi}sin(x-\theta)dx=2\pi\sin(2\pi-\theta)=2\pi\sin(-\theta)=2\pi\sin(\theta)$$

Because $sin(2\pi+x)=sin(x)$ and $\sin(-x)=sin(x)$.
We have:

$$E[X|\theta]=\pi-\sin(\theta)$$
Hence, the Method of Moments estimator of $\theta$ is $E[X|\theta]=\overline{X}=\pi-\sin(\theta)\iff\theta_{moment}=\arcsin(\pi-\overline{X})$

```{r}
theta_methodofmomentsestimator <- asin(pi-3.236842)
theta_methodofmomentsestimator
```

(c)

Now, we will find the MLE for $\theta$ using the Newton-Raphson method with the method of moments estimator for $\theta$ as the starting point. Here's the result - 

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)

NRmethod <- function(p,q,x0,b,maximumvalue) {
  x1 <- x0-p(x0)/q(x0)
  iteration <- 1
  while(abs(x1-x0) > b & iteration < maximumvalue) {
    x0 <- x1
    x1 <- x0-p(x0)/q(x0)
    iteration <- iteration+1
  }
  return(x1)
}

derivativeloglikilihood <- function(theta) {
  return(-sum(sin(-theta)/(1-cos(x-theta))))
}

secondderivativeloglikelihood <- function(theta) {
  return(-sum(1/(1-cos(x-theta))))
}

NRmethod(derivativeloglikelihood, secondderivativeloglikelihood, theta_methodofmomentsestimator, 0.00000001, 500)

```

(d)

Starting with $\theta$ values of -2.7 and 2.7 yield the following answers 
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
NRmethod(derivativeloglikelihood, secondderivativeloglikelihood, -2.7, 0.00000001, 500)
NRmethod(derivativeloglikelihood, secondderivativeloglikelihood, 2.7, 0.00000001, 500)
```
respectively.


# Problem 3

(a)

Here we fit the population growth model to the beetles data using the Gauss-Newton approach - 

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

beetles <- data.frame(
  days    = c(0,  8,  28,  41,  63,  69,   97, 117,  135,  154),
  beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024)
)

growthfunc <- function(t, K, r) {
  2*K/(2+(K-2)*exp(-r*t))
}

n <- nls(beetles~growthfunc(days, K, r), data=beetles, start=list(K=1030, r=0.2),trace=T)
n

```

(b)

The contour plot of the sum of squared errors is produced - 

```{r, eval=TRUE, echo=FALSE, warning=FALSE}

days    = c(0,  8,  28,  41,  63,  69,   97, 117,  135,  154)
beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024)

sumofsquareerrors <- function(K, r) {
  return(sum((beetles - 2*K / (2 + (K - 2) * exp(-r * days)))^2))
}

n <- matrix(0, 100, 100, byrow=T)
for(i in 1:100) {
  for(j in 1:100) {
    K <- 800 + 5 * j
    r <- 0.005 * j
    n[j,i] <- sumofsquareerrors(K, r)
  }
}
K <- seq( 800, 1300, length.out=100)
r <- seq(   0,  0.5, length.out=100)
contour(K, r, n)

```